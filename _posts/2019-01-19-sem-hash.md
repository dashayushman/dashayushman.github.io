---
layout: post
title:  "Subword Semantic Hashing for Intent Classification on Small Datasets"
desc: "In this paper, we introduce the use of Semantic Hashing as embedding for the task of Intent Classification and achieve state-of-the-art performance on three frequently used benchmarks."
keywords: "publication,intent classification,deep learning"
date: 19-01-2019
categories: [publications]
tags: [NLP, featurizer, hashing]
icon: fa-file-text-o
---

### **Status**: Accepted to [IJCNN2019](https://www.ijcnn.org/)
### **Link to the Paper**: [https://arxiv.org/abs/1810.07150](https://arxiv.org/abs/1810.07150)
### **Github Link**: [https://github.com/kumar-shridhar/Know-Your-Intent](https://github.com/kumar-shridhar/Know-Your-Intent)
<br>

# **What is Subword Semantic Hashing**

In this paper, we introduce the use of Semantic Hashing as embedding for the task of Intent Classification and achieve state-of-the-art performance on three frequently used benchmarks. Intent Classification on a small dataset is a challenging task for data-hungry state-of-the-art Deep Learning based systems. Semantic Hashing is an attempt to overcome such a challenge and learn robust text classification. Current word embedding based are dependent on vocabularies. One of the major drawbacks of such methods is out-of-vocabulary terms, especially when having small training datasets and using a wider vocabulary. This is the case in Intent Classification for chatbots, where typically small datasets are extracted from internet communication. Two problems arise by the use of internet communication. First, such datasets miss a lot of terms in the vocabulary to use word embeddings efficiently. Second, users frequently make spelling errors. Typically, the models for intent classification are not trained with spelling errors and it is difficult to think about ways in which users will make mistakes. Models depending on a word vocabulary will always face such issues. An ideal classifier should handle spelling errors inherently. With Semantic Hashing, we overcome these challenges and achieve state-of-the-art results on three datasets: AskUbuntu, Chatbot, and Web Application.
